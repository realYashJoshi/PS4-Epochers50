{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOceGZVvqLZuiAvEcdHMxdb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/realYashJoshi/PS4-Epochers50/blob/main/training_and_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EPOCHERS 50 PS4**"
      ],
      "metadata": {
        "id": "E_8FOHzneVkh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRAINING FILE"
      ],
      "metadata": {
        "id": "NPG93B9fZh6R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIUOqXOkMSys",
        "outputId": "99667118-2aa5-4e72-d686-054e90939159"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actual 'Yes' Count: 6371\n",
            "Actual 'No' Count: 2801\n",
            "Validation Accuracy: 0.70\n",
            "Predicted 'Yes' Count: 8350\n",
            "Predicted 'No' Count: 822\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['vectorizer.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib\n",
        "import re\n",
        "\n",
        "\n",
        "data = pd.read_csv('training_data.csv')\n",
        "\n",
        "\n",
        "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "#Basic text Preprocessing\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "#Count vectorizer or Bag of words method used for feauture extraction\n",
        "vectorizer = CountVectorizer(max_features=5000, stop_words='english')\n",
        "\n",
        "X_train = vectorizer.fit_transform(train_data['Question'] + '[SEP] ' + train_data['Paragraph'])\n",
        "y_train = train_data['Answer_possible']\n",
        "#Here we used a XGB classifier algorithm which works exceptionally well on Tabular data\n",
        "classifier = xgb.XGBClassifier(n_estimators=100, random_state=42)\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "X_val = vectorizer.transform(val_data['Question'] + '[SEP] ' + val_data['Paragraph'])\n",
        "y_val = val_data['Answer_possible']\n",
        "\n",
        "actual_yes_count = np.sum(y_val == 1)\n",
        "actual_no_count = np.sum(y_val == 0)\n",
        "\n",
        "print(f\"Actual 'Yes' Count: {actual_yes_count}\")\n",
        "print(f\"Actual 'No' Count: {actual_no_count}\")\n",
        "\n",
        "\n",
        "val_predictions = classifier.predict(X_val)\n",
        "val_accuracy = accuracy_score(y_val, val_predictions)\n",
        "print(f\"Validation Accuracy: {val_accuracy:.2f}\")\n",
        "\n",
        "\n",
        "num_predicted_yes = np.sum(val_predictions == 1)\n",
        "num_predicted_no = np.sum(val_predictions == 0)\n",
        "\n",
        "print(f\"Predicted 'Yes' Count: {num_predicted_yes}\")\n",
        "print(f\"Predicted 'No' Count: {num_predicted_no}\")\n",
        "#Saving and Exporting the model\n",
        "joblib.dump(classifier, 'xgboost_classifier.pkl')\n",
        "joblib.dump(vectorizer, 'vectorizer.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "INFERENCE FILE"
      ],
      "metadata": {
        "id": "zkNs6_gJZxoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "import joblib\n",
        "import re\n",
        "from transformers import pipeline, DistilBertTokenizer, TFDistilBertForQuestionAnswering\n",
        "import tensorflow as tf\n",
        "\n",
        "#  Here we load the trained XGBoost model and CountVectorizer\n",
        "classifier = joblib.load('xgboost_classifier.pkl')\n",
        "vectorizer = joblib.load('vectorizer.pkl')\n",
        "\n",
        "def preprocess_text(text): #For text Preprocessing\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "# Load the training data\n",
        "training_data = pd.read_csv('training_data.csv')\n",
        "\n",
        "# Select random 500 entries for testing as test data wasnt specified in problem statement\n",
        "test_data = training_data.sample(n=500, random_state=42)\n",
        "\n",
        "# Initialize a DistilBERT tokenizer and model for question answering, this is used for Transer-Learning process which uses SQUAD Model.\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased-distilled-squad')\n",
        "model = TFDistilBertForQuestionAnswering.from_pretrained('distilbert-base-cased-distilled-squad')\n",
        "\n",
        "# Initializing a question answering pipeline\n",
        "qa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Initializing counters\n",
        "count_predictions_yes = 0\n",
        "count_predictions_no = 0\n",
        "\n",
        "# Initializing lists to store the results\n",
        "questions = []\n",
        "paragraphs = []\n",
        "answer_starts = []\n",
        "answer_texts = []\n",
        "\n",
        "# Iterating through the testing data\n",
        "for index, row in test_data.iterrows():\n",
        "    question = row['Question']\n",
        "    paragraph = row['Paragraph']\n",
        "\n",
        "    # Preprocessing the text\n",
        "    question = preprocess_text(question)\n",
        "    paragraph = preprocess_text(paragraph)\n",
        "\n",
        "    # Vectorizing the input using the CountVectorizer\n",
        "    input_text = question + '[SEP] ' + paragraph\n",
        "    X_test = vectorizer.transform([input_text])\n",
        "\n",
        "    # Making predictions using the XGBoost model\n",
        "    prediction = classifier.predict(X_test)[0]\n",
        "\n",
        "    if prediction == 1:\n",
        "        count_predictions_yes += 1\n",
        "        # If the prediction is 'Yes' (1),  we use the question answering pipeline\n",
        "        answer = qa_pipeline(question=question, context=paragraph)\n",
        "\n",
        "        # Extract the answer text and answer_start\n",
        "        answer_text = answer['answer']\n",
        "        answer_start = paragraph.find(answer_text) if answer_text else None\n",
        "    else:\n",
        "        count_predictions_no += 1\n",
        "        # If the prediction is 'No' (0),  we leave the answer fields empty\n",
        "        answer_text = None\n",
        "        answer_start = None\n",
        "\n",
        "    questions.append(question)\n",
        "    paragraphs.append(paragraph)\n",
        "    answer_starts.append(answer_start)\n",
        "    answer_texts.append(answer_text)\n",
        "\n",
        "# To Create DataFrame with the results\n",
        "results_df = pd.DataFrame({\n",
        "    'Question': questions,\n",
        "    'Paragraph': paragraphs,\n",
        "    'Answer_start': answer_starts,\n",
        "    'Answer_text': answer_texts\n",
        "})\n",
        "\n",
        "# Saving the results to a CSV file\n",
        "results_df.to_csv('predictions.csv', index=False)\n",
        "\n",
        "# Printing  the counts of predictions '1' and '0'\n",
        "print(f\"Number of Predictions '1': {count_predictions_yes}\")\n",
        "print(f\"Number of Predictions '0': {count_predictions_no}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnm1z255Z0L4",
        "outputId": "51d0963d-6dc4-47f1-d86a-42e9fe1a8297"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFDistilBertForQuestionAnswering.\n",
            "\n",
            "All the weights of TFDistilBertForQuestionAnswering were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForQuestionAnswering for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Predictions '1': 461\n",
            "Number of Predictions '0': 39\n"
          ]
        }
      ]
    }
  ]
}